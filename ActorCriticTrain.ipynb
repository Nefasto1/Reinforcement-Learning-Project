{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47331800-e1f1-4988-9d31-fd3e7820cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from src.Environment import Environment\n",
    "from src.actor_critic import ActorCritic\n",
    "import torch.distributions as dist\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e80d22f-5ed7-40d7-ae2a-a106982ac3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes, max_steps):\n",
    "    env = Environment()\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset(45,135)\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs, state_value = ac_net(state_tensor)\n",
    "            \n",
    "            # Sample action from the probability distribution\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            # print(action)\n",
    "            \n",
    "            # Take action in the environment\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Compute TD error\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            _, next_state_value = ac_net(next_state_tensor)\n",
    "            td_error = reward + (0.99 * next_state_value * (1 - int(done))) - state_value\n",
    "            \n",
    "            # Compute losses\n",
    "            actor_loss = dist.log_prob(action) * td_error.detach()\n",
    "            critic_loss = td_error * state_value\n",
    "            print(actor_loss,\"..\", critic_loss)\n",
    "            loss = actor_loss*1e11 + critic_loss/1000\n",
    "            \n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        print(f\"Episode {episode+1}, Reward: {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe54d3d8-59d5-406f-acfc-7be90853ceae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5815e-06]], grad_fn=<MulBackward0>) .. tensor([[-6501.2476]], grad_fn=<MulBackward0>)\n",
      "tensor([[2.9068e-06]], grad_fn=<MulBackward0>) .. tensor([[-13973.2412]], grad_fn=<MulBackward0>)\n",
      "tensor([[3.2343e-06]], grad_fn=<MulBackward0>) .. tensor([[-23001.7383]], grad_fn=<MulBackward0>)\n",
      "tensor([[3.5649e-06]], grad_fn=<MulBackward0>) .. tensor([[-33647.9805]], grad_fn=<MulBackward0>)\n",
      "tensor([[3.9007e-06]], grad_fn=<MulBackward0>) .. tensor([[-46033.2500]], grad_fn=<MulBackward0>)\n",
      "tensor([[4.2433e-06]], grad_fn=<MulBackward0>) .. tensor([[-60305.3203]], grad_fn=<MulBackward0>)\n",
      "tensor([[4.5948e-06]], grad_fn=<MulBackward0>) .. tensor([[-76664.9844]], grad_fn=<MulBackward0>)\n",
      "tensor([[4.9585e-06]], grad_fn=<MulBackward0>) .. tensor([[-95425.0234]], grad_fn=<MulBackward0>)\n",
      "tensor([[5.3371e-06]], grad_fn=<MulBackward0>) .. tensor([[-116934.1484]], grad_fn=<MulBackward0>)\n",
      "tensor([[5.7330e-06]], grad_fn=<MulBackward0>) .. tensor([[-141576.5938]], grad_fn=<MulBackward0>)\n",
      "Episode 1, Reward: -186.53196529439376\n",
      "tensor([[6.1483e-06]], grad_fn=<MulBackward0>) .. tensor([[-169801.2812]], grad_fn=<MulBackward0>)\n",
      "tensor([[6.5855e-06]], grad_fn=<MulBackward0>) .. tensor([[-202136.0625]], grad_fn=<MulBackward0>)\n",
      "tensor([[7.0473e-06]], grad_fn=<MulBackward0>) .. tensor([[-239206.2656]], grad_fn=<MulBackward0>)\n",
      "tensor([[7.5362e-06]], grad_fn=<MulBackward0>) .. tensor([[-281732.4062]], grad_fn=<MulBackward0>)\n",
      "tensor([[8.0550e-06]], grad_fn=<MulBackward0>) .. tensor([[-330533.9062]], grad_fn=<MulBackward0>)\n",
      "tensor([[8.6062e-06]], grad_fn=<MulBackward0>) .. tensor([[-386528.7500]], grad_fn=<MulBackward0>)\n",
      "tensor([[9.1920e-06]], grad_fn=<MulBackward0>) .. tensor([[-450731.0938]], grad_fn=<MulBackward0>)\n",
      "tensor([[9.8145e-06]], grad_fn=<MulBackward0>) .. tensor([[-524251.0312]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.0476e-05]], grad_fn=<MulBackward0>) .. tensor([[-608312.0625]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.1177e-05]], grad_fn=<MulBackward0>) .. tensor([[-704221.3750]], grad_fn=<MulBackward0>)\n",
      "Episode 2, Reward: -186.53196529439376\n",
      "tensor([[1.1921e-05]], grad_fn=<MulBackward0>) .. tensor([[-813406.9375]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.2707e-05]], grad_fn=<MulBackward0>) .. tensor([[-937442.7500]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.3539e-05]], grad_fn=<MulBackward0>) .. tensor([[-1078017.5000]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.4417e-05]], grad_fn=<MulBackward0>) .. tensor([[-1237003.7500]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.5343e-05]], grad_fn=<MulBackward0>) .. tensor([[-1416426.7500]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.6318e-05]], grad_fn=<MulBackward0>) .. tensor([[-1618475.2500]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.7345e-05]], grad_fn=<MulBackward0>) .. tensor([[-1845557.8750]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.8424e-05]], grad_fn=<MulBackward0>) .. tensor([[-2100273.5000]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.9557e-05]], grad_fn=<MulBackward0>) .. tensor([[-2385377.7500]], grad_fn=<MulBackward0>)\n",
      "tensor([[2.0746e-05]], grad_fn=<MulBackward0>) .. tensor([[-2703910.2500]], grad_fn=<MulBackward0>)\n",
      "Episode 3, Reward: -186.53196529439376\n",
      "tensor([[2.1992e-05]], grad_fn=<MulBackward0>) .. tensor([[-3059134.]], grad_fn=<MulBackward0>)\n",
      "tensor([[2.3296e-05]], grad_fn=<MulBackward0>) .. tensor([[-3454544.7500]], grad_fn=<MulBackward0>)\n",
      "tensor([[2.4662e-05]], grad_fn=<MulBackward0>) .. tensor([[-3893931.2500]], grad_fn=<MulBackward0>)\n",
      "tensor([[2.6089e-05]], grad_fn=<MulBackward0>) .. tensor([[-4381302.]], grad_fn=<MulBackward0>)\n",
      "tensor([[2.7579e-05]], grad_fn=<MulBackward0>) .. tensor([[-4920945.]], grad_fn=<MulBackward0>)\n",
      "tensor([[2.9135e-05]], grad_fn=<MulBackward0>) .. tensor([[-5517568.]], grad_fn=<MulBackward0>)\n",
      "tensor([[3.0758e-05]], grad_fn=<MulBackward0>) .. tensor([[-6176029.5000]], grad_fn=<MulBackward0>)\n",
      "tensor([[3.2448e-05]], grad_fn=<MulBackward0>) .. tensor([[-6901494.5000]], grad_fn=<MulBackward0>)\n",
      "tensor([[3.4209e-05]], grad_fn=<MulBackward0>) .. tensor([[-7699761.]], grad_fn=<MulBackward0>)\n",
      "tensor([[3.6041e-05]], grad_fn=<MulBackward0>) .. tensor([[-8576641.]], grad_fn=<MulBackward0>)\n",
      "Episode 4, Reward: -186.53196529439376\n",
      "tensor([[3.7945e-05]], grad_fn=<MulBackward0>) .. tensor([[-9538341.]], grad_fn=<MulBackward0>)\n",
      "tensor([[3.9924e-05]], grad_fn=<MulBackward0>) .. tensor([[-10591679.]], grad_fn=<MulBackward0>)\n",
      "tensor([[4.1979e-05]], grad_fn=<MulBackward0>) .. tensor([[-11743693.]], grad_fn=<MulBackward0>)\n",
      "tensor([[4.4110e-05]], grad_fn=<MulBackward0>) .. tensor([[-13001631.]], grad_fn=<MulBackward0>)\n",
      "tensor([[4.6321e-05]], grad_fn=<MulBackward0>) .. tensor([[-14373677.]], grad_fn=<MulBackward0>)\n",
      "tensor([[4.8611e-05]], grad_fn=<MulBackward0>) .. tensor([[-15867995.]], grad_fn=<MulBackward0>)\n",
      "tensor([[5.0983e-05]], grad_fn=<MulBackward0>) .. tensor([[-17493322.]], grad_fn=<MulBackward0>)\n",
      "tensor([[5.3438e-05]], grad_fn=<MulBackward0>) .. tensor([[-19258852.]], grad_fn=<MulBackward0>)\n",
      "tensor([[5.5978e-05]], grad_fn=<MulBackward0>) .. tensor([[-21174256.]], grad_fn=<MulBackward0>)\n",
      "tensor([[5.8603e-05]], grad_fn=<MulBackward0>) .. tensor([[-23249692.]], grad_fn=<MulBackward0>)\n",
      "Episode 5, Reward: -186.53196529439376\n"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "ac_net = ActorCritic(7,4)\n",
    "optimizer = optim.Adam(ac_net.parameters(), lr = 1e-2)\n",
    "train(n_episodes=5, max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba2809-e5ab-40e6-ae58-9a40c6d092db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
